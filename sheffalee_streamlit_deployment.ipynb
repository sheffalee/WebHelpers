{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Streamlit deployment\n",
        "\n",
        "i have used roberta model to build the sentiment analysis model ,and i have used openai GPT 3.5 for building the chatbot. Then i have deployed it\n",
        "using the streamlit app using localtunnel package."
      ],
      "metadata": {
        "id": "rF8XZDrd52_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing all the necessary packages"
      ],
      "metadata": {
        "id": "Xphch6Rn4o9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q"
      ],
      "metadata": {
        "id": "qsuWCsNNibfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "5kdoFPoWmXj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "KLiR3E-CzkGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit_chat"
      ],
      "metadata": {
        "id": "Lp-i7zfezEAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spicy"
      ],
      "metadata": {
        "id": "73ag_avyzsCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing the chatbot.py file"
      ],
      "metadata": {
        "id": "qjgmCZaU4xnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chatbot.py\n",
        "# Import the openai library\n",
        "import openai\n",
        "\n",
        "# Define a function to generate a response from OpenAI's GPT-3 model\n",
        "def generate_response(prompt):\n",
        "    # Create a completion using the GPT-3 engine\n",
        "    completions = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",  # Choose the GPT-3 engine\n",
        "        prompt=prompt,              # The input prompt for the model\n",
        "        max_tokens=1024,            # Maximum number of tokens in the response\n",
        "        n=1,                        # Generate only one response\n",
        "        stop=None,                  # No stopping criteria\n",
        "        temperature=0.5,            # Control the randomness of the response\n",
        "    )\n",
        "\n",
        "    # Get the generated text from the response\n",
        "    message = completions.choices[0].text\n",
        "    return message\n"
      ],
      "metadata": {
        "id": "fFXtRI8TxxCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing the app.py file"
      ],
      "metadata": {
        "id": "m6Y5rd1d43dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# Import required libraries\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
        "from streamlit_chat import message  # Import the message function from streamlit_chat\n",
        "from chatbot import generate_response  # Import the generate_response function from chatbot.py\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "# Define a function to preprocess text\n",
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "# Define sentiment analysis model and tokenizer\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "# Define a function to predict sentiment\n",
        "def predict_sentiment(text):\n",
        "    text = preprocess(text)\n",
        "    encoded_input = tokenizer(text, return_tensors='pt')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0][0].detach().numpy()\n",
        "    scores = softmax(scores)\n",
        "    ranking = np.argsort(scores)\n",
        "    highest_sentiment = config.id2label[ranking[-1]]\n",
        "    return highest_sentiment\n",
        "\n",
        "def main():\n",
        "    # Set the title for the Streamlit app\n",
        "    st.title(\"WebHelpers App\")\n",
        "\n",
        "    # Sentiment Analysis Section\n",
        "    st.title(\"Sentiment Analysis\")\n",
        "    # User input for sentiment analysis\n",
        "    user_input_sentiment = st.text_input(\"Enter your reviews here\")\n",
        "\n",
        "    if user_input_sentiment:\n",
        "        # Perform sentiment analysis\n",
        "        highest_sentiment = predict_sentiment(user_input_sentiment)\n",
        "\n",
        "        # Display highest sentiment prediction\n",
        "        st.write(f\"Sentiment: {highest_sentiment.capitalize()}\")\n",
        "\n",
        "    # Chatbot Section\n",
        "    st.title(\"Chatbot\")\n",
        "    # User input for chatbot\n",
        "    user_input_chatbot = st.text_input(\"Enter your queries here\")\n",
        "\n",
        "    if user_input_chatbot:\n",
        "        output = generate_response(user_input_chatbot)\n",
        "        st.write(\"Chatbot Response:\")\n",
        "        st.write(output)\n",
        "\n",
        "# Run the Streamlit app\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZnGm4YguuC6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is used for finding the endpoint ip address\n",
        "\n",
        "copy this address and paste in the page after you click the streamlit app url link."
      ],
      "metadata": {
        "id": "K4wsya-P489W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com\n"
      ],
      "metadata": {
        "id": "373o2634ibiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "Klk1b6hUMjfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qkbgdjTEMXbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "running the streamlit app from the colab notebook using localtunnel\n",
        "\n",
        "if you face any issues related to localtunnel you can refer this:https://github.com/localtunnel/localtunnel"
      ],
      "metadata": {
        "id": "Be76GIWk5Wx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
       
      ],
      "metadata": {
        "id": "AlPVpudaPDw2"
      }
    }
  ]
}
